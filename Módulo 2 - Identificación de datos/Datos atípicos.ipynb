{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: left;;\" src='Figures/iteso.jpg' width=\"100\" height=\"200\"/></a>\n",
    "\n",
    "# <center> <font color= #000047> Datos Atípicos </font> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Qué son los valores atípicos (outliers)?\n",
    "\n",
    "Un valor atípico es una observación que es diferente a las demás. Es raro, o distinto, o no encaja de alguna manera. También se les llama anomalías.\n",
    "\n",
    "Los valores atípicos pueden suceder por varias causas, tales como:\n",
    "\n",
    "- Error de medición o entrada.\n",
    "- Corrupción de datos.\n",
    "- Observación verdaderamente atípica.\n",
    "\n",
    "Incluir valores atípicos en modelos basados en datos puede ser riesgoso. La existencia de un solo valor extremo engañoso tiene el potencial de cambiar la conclusión que implica el modelo. Por lo tanto, es importante gestionar ese tipo de riesgo.\n",
    "\n",
    "> No hay una forma precisa de definir e identificar valores atípicos en general debido a las particularidades de cada conjunto de datos. En su lugar, se puede interpretar las observaciones y decidir si un valor es atípico o no.\n",
    "\n",
    "> Podemos usar métodos estadísticos para identificar observaciones que parecen ser raras o poco probables dado los datos disponibles. Esto no significa que los valores identificados sean atípicos y deban eliminarse.\n",
    "\n",
    "> Un buen consejo es considerar graficar los valores atípicos identificados, quizás en el contexto de los valores no atípicos para ver si hay relaciones o patrones sistemáticos. Si los hay, tal vez no sean atípicos y puedan explicarse, o tal vez los propios valores atípicos puedan identificarse de manera más sistemática.\n",
    "\n",
    "#### Tipos de Valores atípicos\n",
    "\n",
    "Un valor atípico puede ser de dos tipos:\n",
    "\n",
    "1. Univariado\n",
    "2. Multivariado\n",
    "\n",
    "Los valores atípicos univariados pueden encontrarse al observar la distribución de una sola variable. Los valores atípicos multivariados son valores atípicos en un espacio n-dimensional. Para encontrarlos, debes observar distribuciones en varias dimensiones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 20\n",
    "x = np.linspace(0, 10, N)\n",
    "y = 10 + 2 * x + np.random.normal(loc=0, scale=2, size=(N,))\n",
    "y[0] = 30\n",
    "y[-1] = 10\n",
    "\n",
    "df_xy = pd.DataFrame({'x': x, 'y': y})\n",
    "#distribuciones de x, y\n",
    "plt.figure(figsize=(6,4))\n",
    "df_xy.boxplot(figsize=(6,5));\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relación entre x, y\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(x,y,'xr', label='datos')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existen diversas técnicas para identificar valores atípicos en un conjunto de datos, incluyendo métodos de **inspección visual, métodos estadísticos e incluso modelos de machine learning como el Local Outlier Factor (LOF), el Isolation Forest (IForest)**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers Univariados\n",
    "\n",
    "Las metodologías más comunes para detectarlos son:\n",
    "\n",
    "1. Método del Rango Intercuantílico (IQR)\n",
    "2. Método de la desviación estándar\n",
    "3. Método de la puntuación Z Z-score modificada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Método del Rango Intercuantílico (IQR)\n",
    "\n",
    "El concepto de rango intercuartílico (IQR) se utiliza para construir los diagramas de caja. El IQR es un concepto en estadística que se utiliza para medir la dispersión y variabilidad de los datos dividiendo el conjunto en cuartiles.\n",
    "\n",
    "Cualquier conjunto de datos u observaciones se divide en cuatro intervalos definidos según los valores de los datos y cómo se comparan con el conjunto completo. Un cuartil es lo que divide los datos en tres puntos y cuatro intervalos.\n",
    "\n",
    "$IQR$ es la diferencia entre el tercer cuartil y el primer cuartil ($IQR = Q_3 - Q_1$). Los valores atípicos en este caso se definen como las observaciones que están por debajo de ($Q1 − 1.5*IQR$) o el bigote inferior del diagrama de caja, o por encima de ($Q3 + 1.5* IQR$) o el bigote superior. Puede representarse visualmente mediante el diagrama de caja."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 1620,
     "status": "ok",
     "timestamp": 1675822725909,
     "user": {
      "displayName": "Carlos Arellano",
      "userId": "03055712073717640513"
     },
     "user_tz": 360
    },
    "id": "DNB5ZKAQz4gc",
    "outputId": "cd7e4819-03ef-42a8-e5ed-9b1d7960fda2"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "LI=load_iris()\n",
    "df=pd.DataFrame(LI.data,columns=LI.feature_names)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sepal width (cm)'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 173,
     "status": "ok",
     "timestamp": 1675822851521,
     "user": {
      "displayName": "Carlos Arellano",
      "userId": "03055712073717640513"
     },
     "user_tz": 360
    },
    "id": "_LTHTUdU0WOO",
    "outputId": "7ea4bf6e-5de7-423f-bfe2-68dd522055fd"
   },
   "outputs": [],
   "source": [
    "q3,q1=np.quantile(df,(0.75,0.25),axis=0)\n",
    "iqr=q3-q1\n",
    "iqr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 162,
     "status": "ok",
     "timestamp": 1675822936080,
     "user": {
      "displayName": "Carlos Arellano",
      "userId": "03055712073717640513"
     },
     "user_tz": 360
    },
    "id": "U_J8ICNk0znT",
    "outputId": "f6c6252d-a9ce-42f4-9f4b-a88e08b7a6b3"
   },
   "outputs": [],
   "source": [
    "Li=q1-1.5*iqr\n",
    "Ls=q3+1.5*iqr\n",
    "Li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 227,
     "status": "ok",
     "timestamp": 1675822939264,
     "user": {
      "displayName": "Carlos Arellano",
      "userId": "03055712073717640513"
     },
     "user_tz": 360
    },
    "id": "WsNRqfxm1K4h",
    "outputId": "6ada192f-0583-4109-ad17-49388a544c03"
   },
   "outputs": [],
   "source": [
    "Ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df.iloc[:,1] < Li[1]) | (df.iloc[:,1] > Ls[1] ) #Outliers para sepal width (cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spel_width_outliers = df[(df.iloc[:,1] < Li[1]) | (df.iloc[:,1] > Ls[1] )]['sepal width (cm)'] #Outliers para sepal length (cm)\n",
    "spel_width_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spel_width_woutliers = df[~((df.iloc[:,1] < Li[1]) | (df.iloc[:,1] > Ls[1] ))]['sepal width (cm)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spel_width_woutliers.hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MetodoIQR (df,n,features):\n",
    "    outlier_list = []\n",
    "    \n",
    "    for column in features:\n",
    "                \n",
    "        # 1st quartile (25%)\n",
    "        Q1 = np.percentile(df[column], 25)\n",
    "        # 3rd quartile (75%)\n",
    "        Q3 = np.percentile(df[column],75)\n",
    "        \n",
    "        # Interquartile range (IQR)\n",
    "        IQR = Q3 - Q1\n",
    "        \n",
    "        # limite iqr\n",
    "        outlier_limit = 1.5 * IQR\n",
    "        \n",
    "        # determinar la lista de outliers\n",
    "        outlier_list_column = df[(df[column] < Q1 - outlier_limit) | (df[column] > Q3 + outlier_limit )].index\n",
    "        \n",
    "        # agregando a la lista de outliers\n",
    "        outlier_list.extend(outlier_list_column)\n",
    "        \n",
    "    # seleccionar las observaciones que contienen más de cierto numero de outliers\n",
    "    outlier_list = Counter(outlier_list)\n",
    "    print(outlier_list)\n",
    "    multiple_outliers = list( k for k, v in outlier_list.items() if v >= n )\n",
    "    \n",
    "    return multiple_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers_index = MetodoIQR(df, 1, df.columns)\n",
    "outliers_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Método de la desviación estándar\n",
    "\n",
    "Si sabemos que la distribución de los valores en la muestra es gaussiana o similar a la gaussiana, podemos usar la desviación estándar de la muestra como un límite para identificar valores atípicos.\n",
    "\n",
    "La desviación estándar muestra cuánto se dispersan los puntos de datos individuales respecto a la media. Si la distribución de los datos es normal entonces:\n",
    "* El 68% de los valores de los datos se encuentran dentro de una desviación estándar de la media\n",
    "* El 95% están dentro de dos desviaciones estándar\n",
    "* El 99.7% se encuentran dentro de tres desviaciones estándar.\n",
    "\n",
    "Dependiendo de la especificación establecida, ya sea a 2 o 3 veces la desviación estándar, podemos detectar y eliminar valores atípicos del conjunto de datos.\n",
    "\n",
    "Este método puede fallar en la detección de valores atípicos porque los valores atípicos aumentan la desviación estándar. Cuanto más extremo sea el valor atípico, más se ve afectada la desviación estándar.\n",
    "\n",
    "<img src=\"Figures/Standard_deviation_diagram.svg\" width=\"800\" height=\"800\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# El conjunto de datos contiene transacciones realizadas con tarjetas de crédito. Contiene únicamente variables de entrada numéricas que son el resultado de una transformación PCA.\n",
    "df_credit = pd.read_csv('Data/creditcard.csv') \n",
    "df_credit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_credit.columns[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list = df_credit.columns[:-1].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ploteamos los diagramas de caja de algunas características\n",
    "fig, axes = plt.subplots(nrows=2, ncols=3,figsize=(12,8))\n",
    "fig.suptitle('Diagrama de caja Características vs Class\\n', size = 18)\n",
    "\n",
    "sns.boxplot(ax=axes[0, 0], data=df, x='Class', y='V17')\n",
    "axes[0,0].set_title(\"V17\");\n",
    "\n",
    "sns.boxplot(ax=axes[0, 1], data=df, x='Class', y='V10')\n",
    "axes[0,1].set_title(\"V10\");\n",
    "\n",
    "sns.boxplot(ax=axes[0, 2], data=df, x='Class', y='V12')\n",
    "axes[0,2].set_title(\"V12\");\n",
    "\n",
    "sns.boxplot(ax=axes[1, 0], data=df, x='Class', y='V16')\n",
    "axes[1,0].set_title(\"V16\");\n",
    "\n",
    "sns.boxplot(ax=axes[1, 1], data=df, x='Class', y='V14')\n",
    "axes[1,1].set_title(\"V14\");\n",
    "\n",
    "sns.boxplot(ax=axes[1, 2], data=df, x='Class', y='V3')\n",
    "axes[1,2].set_title(\"V3\");\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mean, data_std = df['V10'].mean(), df['V10'].std()\n",
    "cut_off = data_std * 3\n",
    "lower, upper = data_mean - cut_off, data_mean + cut_off\n",
    "\n",
    "print('The lower bound value is:', data_mean - cut_off)\n",
    "print('The upper bound value is:', data_mean + cut_off)\n",
    "\n",
    "plt.figure(figsize = (10,5))\n",
    "sns.histplot(x = 'V11', data=df, bins=70)\n",
    "plt.axvspan(xmin = lower,xmax= df.V10.min(),alpha=0.2, color='red')\n",
    "plt.axvspan(xmin = upper,xmax= df.V10.max(),alpha=0.2, color='red')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Metodo_StDev(df,n,features):\n",
    "    outlier_indices = []\n",
    "    \n",
    "    for column in features:\n",
    "        # calcula la media y la desviación estándar de todo el dataframe\n",
    "        data_mean = df[column].mean()\n",
    "        data_std = df[column].std()\n",
    "        \n",
    "        # calcula el corte en la desviación estandar\n",
    "        cut_off = data_std * 3\n",
    "        \n",
    "        # Determina la lista de indices de los outliers en cada columns\n",
    "        outlier_list_column = df[(df[column] < data_mean - cut_off) | (df[column] > data_mean + cut_off)].index\n",
    "        \n",
    "        #Agrega los indices de los outliers obtenidos \n",
    "        outlier_indices.extend(outlier_list_column)\n",
    "        \n",
    "    # Selecciona las observaciones que contienen más de x outliers\n",
    "    outlier_ind = Counter(outlier_indices)        \n",
    "    multiple_outliers = list( k for k, v in outlier_ind.items() if v > n )\n",
    "    \n",
    "    return multiple_outliers   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers_StDev = Metodo_StDev(df,1,df.columns) #Indices que representan outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_credit_wout = df.drop(outliers_StDev, axis = 0).reset_index(drop=True)\n",
    "df_credit_wout.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=3, ncols=3,figsize=(13,8))\n",
    "fig.suptitle('Algunas distribuciones después de aplicar el método de desviación estandar\\n', size = 18)\n",
    "\n",
    "axes[0,0].hist(df_credit_wout['V17'], bins=60, linewidth=0.5, edgecolor=\"black\")\n",
    "axes[0,0].set_title(\"V17\");\n",
    "\n",
    "axes[0,1].hist(df_credit_wout['V10'], bins=60, linewidth=0.5, edgecolor=\"black\")\n",
    "axes[0,1].set_title(\"V10\");\n",
    "\n",
    "axes[0,2].hist(df_credit_wout['V12'], bins=60, linewidth=0.5, edgecolor=\"black\")\n",
    "axes[0,2].set_title(\"V12\");\n",
    "\n",
    "axes[1,0].hist(df_credit_wout['V16'], bins=60, linewidth=0.5, edgecolor=\"black\")\n",
    "axes[1,0].set_title(\"V16\");\n",
    "\n",
    "axes[1,1].hist(df_credit_wout['V14'], bins=60, linewidth=0.5, edgecolor=\"black\")\n",
    "axes[1,1].set_title(\"V14\");\n",
    "\n",
    "axes[1,2].hist(df_credit_wout['V3'], bins=60, linewidth=0.5, edgecolor=\"black\")\n",
    "axes[1,2].set_title(\"V3\");\n",
    "\n",
    "axes[2,0].hist(df_credit_wout['V7'], bins=60, linewidth=0.5, edgecolor=\"black\")\n",
    "axes[2,0].set_title(\"V7\");\n",
    "\n",
    "axes[2,1].hist(df_credit_wout['V11'], bins=60, linewidth=0.5, edgecolor=\"black\")\n",
    "axes[2,1].set_title(\"V11\");\n",
    "\n",
    "axes[2,2].hist(df_credit_wout['V4'], bins=60, linewidth=0.5, edgecolor=\"black\")\n",
    "axes[2,2].set_title(\"V4\");\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Método de puntuación Z (Z-score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al calcular el **Z-Score** (puntuación Z) se utiliza para convertir los datos en otro conjunto con media = 0, describe la posición de un valor bruto en términos de su distancia respecto a la media, medida en unidades de desviación estándar.\n",
    "\n",
    "Esta técnica asume una **distribución gaussiana** de los datos. Los valores atípicos serán los datos que están en las colas de la distribución. En la mayoría de los casos se utiliza un umbral de 3 o -3, es decir, si el valor de la puntuación Z es mayor o menor que 3 o -3 respectivamente, ese punto de datos será identificado como valor atípico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_taxis = sns.load_dataset('taxis')\n",
    "df_taxis.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usando un método de visualización (histograma)\n",
    "plt.figure(figsize=(5,4))\n",
    "sns.histplot(data=df_taxis, x = 'total', bins=60, linewidth=0.5, edgecolor=\"black\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_score =  (df_taxis['total'] - df_taxis['total'].mean())/df_taxis['total'].std()\n",
    "z_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,4))\n",
    "sns.histplot(data=z_score, bins=60, linewidth=0.5, edgecolor=\"black\")\n",
    "plt.title(\"Datos transformados por z-score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_score_abs = abs(z_score)\n",
    "outliers_zscore = z_score_abs > 3 #+- 3 veces\n",
    "outliers_zscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_taxis[~outliers_zscore]['total'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,4))\n",
    "sns.histplot(data=df_taxis[~outliers_zscore]['total'], bins=60, linewidth=0.5, edgecolor=\"black\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Metodo_Z_score (df,n,features):\n",
    "    outlier_list = []\n",
    "    \n",
    "    for column in features:\n",
    "        # Calculando la media y la desviación estándar de cada característica\n",
    "        data_mean = df[column].mean()\n",
    "        data_std = df[column].std()\n",
    "        threshold = 3\n",
    "        \n",
    "        z_score = abs( (df[column] - data_mean)/data_std )\n",
    "        \n",
    "        # Determinando los indices que corresponden a los outliers \n",
    "        outlier_list_column =  df[z_score > threshold].index\n",
    "        \n",
    "        # Añadiendo los indices que corresponden a los outliers\n",
    "        outlier_list.extend(outlier_list_column)\n",
    "        \n",
    "    # Seleccionando los indices que cumplen con un numero n de veces que aparecen como outliers en cada columna\n",
    "    outlier_list = Counter(outlier_list)        \n",
    "    multiple_outliers = list( k for k, v in outlier_list.items() if v > n )\n",
    "    \n",
    "    return multiple_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_taxis.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_taxis.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_taxis.select_dtypes(include='number').head(1) # Columnas numericas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_caracteristicas = df_taxis.select_dtypes(include='number').columns.to_list()\n",
    "lista_caracteristicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers_z_score = Metodo_Z_score(df_taxis,1,lista_caracteristicas)\n",
    "\n",
    "# dropping outliers\n",
    "df_woutliers_taxi = df_taxis.drop(outliers_z_score, axis = 0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_woutliers_taxi.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import zscore\n",
    "\n",
    "z_scores = zscore(df_taxis['total'])\n",
    "abs_z_scores = np.abs(z_scores)\n",
    "df_taxis[abs_z_scores >3]['total']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La elección de 3 como umbral proviene de la regla empírica, según la cual los datos dentro de 3 veces la desviación estándar respecto a la media representan el 99.7% de los datos de la distribución. Sabiendo esto, podemos concluir con bastante seguridad que los datos que caen más allá de este umbral son atípicos, pues son distintos al 99.7% de los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Z-Score Modificado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "Cuando los datos son asimétricos o no se distribuyen de forma normal podemos utilizar el **z-score modificado (MAD-Z Score)**. el z-score modificado mide cuánto se aleja un valor de la mediana en términos de la desviación absoluta mediana.\n",
    "\n",
    "$$ M_i = \\frac{0.6745*(x_i - Mediana)}{MAD}$$\n",
    "\n",
    "donde:\n",
    "- $x_i$: Un valor de dato individual\n",
    "- $Mediana$: La mediana del conjunto de datos\n",
    "- $MAD$: La desviación absoluta mediana del conjunto de datos\n",
    "\n",
    "La desviación absoluta mediana ($MAD$) es una estadística robusta de variabilidad que mide la dispersión de un conjunto de datos. Es menos afectada por valores atípicos que otras medidas de dispersión como la desviación estándar y la varianza. \n",
    "\n",
    "Si los datos son normales, la desviación estándar suele ser la mejor opción para evaluar la dispersión. Sin embargo, si los datos no son normales, el MAD es una estadística que puedes usar en su lugar.\n",
    "\n",
    "$$MAD = Mediana(|x_i – x_m|)$$\n",
    "\n",
    "donde:\n",
    "- $x_i$: El i-ésimo valor en el conjunto de datos\n",
    "- $x_m$: El valor mediano en el conjunto de datos\n",
    "\n",
    "**Ejemplo:** Considere los datos $(1, 1, 2, 2 , 4, 6, 9)$. Su mediana es $2$. Las desviaciones absolutas con respecto a $2$ son $(1, 1, 0, 0, 2, 4, 7)$, que a su vez tienen una mediana de $1$ (ya que las desviaciones absolutas ordenadas son $(0, 0, 1, 1 , 2, 4, 7)$). Por lo tanto, la desviación absoluta mediana de estos datos es $1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_taxis.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median = df_taxis['total'].median() #caluclando la mediana\n",
    "median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_diff = (df_taxis['total'] - median).abs() #desviaciones absolutas con respecto a la mediana\n",
    "abs_diff.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import median_abs_deviation #Usanndo la librería de python\n",
    "mad_score = median_abs_deviation(df_taxis['total'])\n",
    "mad_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Metodo_Z_ScoreMod (df,n,features):\n",
    "    outlier_list = []\n",
    "    \n",
    "    for column in features:\n",
    "        # Calculando la media y la desviación estándar de cada característica\n",
    "        data_mean = df[column].mean()\n",
    "        data_std = df[column].std()\n",
    "        threshold = 3\n",
    "        MAD = median_abs_deviation #función de scipy.stats\n",
    "        \n",
    "        mod_z_score = abs(0.6745*(df[column] - data_mean)/MAD(df[column]) )\n",
    "                \n",
    "        # determinando la lista de los indices de outliers por cada característica   \n",
    "        outlier_list_column =  df[mod_z_score >threshold].index\n",
    "        \n",
    "        # agregando los indices de los outliers de cada columna a la lista\n",
    "        outlier_list.extend(outlier_list_column)\n",
    "        \n",
    "    # seleccionando las observaciones que contengan más de n número de outliers\n",
    "    outlier_list = Counter(outlier_list)    \n",
    "    #print(outlier_list)\n",
    "    multiple_outliers = list( k for k, v in outlier_list.items() if v > n )\n",
    "    \n",
    "    return multiple_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detectando outliers mediante el método z-score modificado\n",
    "outliers_z_score = Metodo_Z_ScoreMod(df_taxis,2,lista_caracteristicas)\n",
    "\n",
    "# eliminando los outliers del dataframe\n",
    "df_out_zscoremod_taxis = df_taxis.drop(outliers_z_score, axis = 0).reset_index(drop=True)\n",
    "df_out_zscoremod_taxis.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_out_zscoremod_taxis), len(df_taxis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pyod\n",
    "from pyod.models.mad import MAD\n",
    "mad = MAD(threshold = 3)\n",
    "lab = mad.fit(df_taxis['total'].values.reshape(-1,1)).labels_\n",
    "lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_taxis_woutliers = df_taxis[lab == 0]\n",
    "df_taxis_woutliers.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_taxis_woutliers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers Multivariados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Son valores atípicos detectados considerando la relación entre dos o más variables. Para este caso, se utilizan algoritmos de machine learning que ayudad a la detección de outliers, algunas metodologías más comunes para detectar valores atípicos multivariados son:\n",
    "\n",
    "1. Local Outlier Factor (LOF)\n",
    "2. Isolation Forest\n",
    "3. DBSCAN\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Outlier Factor (LOF)\n",
    "\n",
    "El método Local Outlier Factor (LOF) es un algoritmo no supervisado basado en la densidad local para detectar anomalías. LOF compara la densidad local de un punto con la de sus vecinos para determinar si es un outlier.\n",
    "\n",
    "[LOF paper](https://dl.acm.org/doi/pdf/10.1145/335191.335388)\n",
    "\n",
    "**¿Cómo funciona LOF?**\n",
    "\n",
    "LOF mide la rareza de un punto calculando la densidad local de sus vecinos más cercanos. Si la densidad de un punto es significativamente menor que la de sus vecinos, se considera un outlier.\n",
    "\n",
    "El algoritmo sigue estos pasos:\n",
    "\n",
    "1. Para cada punto, encuentra sus k vecinos más cercanos.\n",
    "2. Calcula la distancia de alcance de cada punto respecto a sus vecinos.\n",
    "3. Calcula la densidad local de cada punto.\n",
    "4. Calcula el factor LOF como la razón entre la densidad de los vecinos y la densidad del punto.\n",
    "\n",
    "Un valor LOF cercano a 1 indica que el punto tiene una densidad similar a la de sus vecinos (no es un outlier). Un valor mucho mayor que 1 indica que el punto es un outlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "# Generar datos normales\n",
    "np.random.seed(42)\n",
    "X_inliers = 0.3 * np.random.randn(100, 2)\n",
    "X_inliers = np.r_[X_inliers + 2, X_inliers - 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar outliers\n",
    "X_outliers = np.random.uniform(low=-4, high=4, size=(20, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unir datos\n",
    "X = np.r_[X_inliers, X_outliers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustar LOF\n",
    "clf = LocalOutlierFactor(n_neighbors=20, contamination=0.1)\n",
    "y_pred = clf.fit_predict(X)\n",
    "outliers = y_pred == -1\n",
    "\n",
    "# Visualización\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(X[:, 0], X[:, 1], color='b', s=20, label='Normal')\n",
    "plt.scatter(X[outliers, 0], X[outliers, 1], color='r', s=40, label='Outlier')\n",
    "plt.title('Detección de anomalías con LOF')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ejemplo dataser heart\n",
    "df_heart = pd.read_csv(\"Data/heart.csv\")\n",
    "df_heart.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "clf = LocalOutlierFactor(n_neighbors=50, contamination='auto')\n",
    "X = df_heart[['Age','Chol']].values\n",
    "y_pred = clf.fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "in_mask = [True if l == 1 else False for l in y_pred]\n",
    "out_mask = [True if l == -1 else False for l in y_pred]\n",
    "\n",
    "plt.title(\"Local Outlier Factor (LOF)\")\n",
    "\n",
    "a = plt.scatter(X[in_mask, 0], X[in_mask, 1], c = 'blue',\n",
    "                edgecolor = 'k', s = 30)\n",
    "# outliers\n",
    "b = plt.scatter(X[out_mask, 0], X[out_mask, 1], c = 'red',\n",
    "                edgecolor = 'k', s = 30)\n",
    "plt.axis('tight')\n",
    "plt.xlabel('Age');\n",
    "plt.ylabel('Cholestrol');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- LOF Detecta outliers locales y globales. No requiere suposiciones sobre la distribución de los datos.\n",
    "\n",
    "**Sin embargo:**\n",
    "- Es sensible a la elección de k (número de vecinos).\n",
    "- Puede ser costoso computacionalmente para grandes volúmenes de datos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Isolation Forest\n",
    "\n",
    "Isolation Forest (IF) es similar a Random Forest y se basa en árboles de decisión. No hay etiquetas predefinidas aquí. Es un algoritmo de aprendizaje no supervisado que identifica anomalías aislando valores atípicos en los datos.\n",
    "\n",
    "Divide los datos usando un conjunto de árboles y proporciona una puntuación de anomalía observando cuán aislado está el punto en la estructura encontrada. La puntuación de anomalía se utiliza para diferenciar los valores atípicos de las observaciones normales. Un concepto importante en este método es el número de aislamiento. El número de aislamiento es el número de divisiones necesarias para aislar un punto de datos. Este número de divisiones se determina siguiendo estos pasos:\n",
    "\n",
    "- Se selecciona aleatoriamente un punto “a” para aislar.\n",
    "- Se selecciona aleatoriamente un punto de datos “b” que esté entre el valor mínimo y máximo y que sea diferente de “a”.\n",
    "- Si el valor de “b” es menor que el de “a”, “b” se convierte en el nuevo límite inferior.\n",
    "- Si el valor de “b” es mayor que el de “a”, “b” se convierte en el nuevo límite superior.\n",
    "- Este procedimiento se repite mientras haya puntos de datos distintos de “a” entre el límite superior e inferior.\n",
    "\n",
    "Se requieren menos divisiones para aislar un valor atípico que para aislar un valor no atípico, es decir, un valor atípico tiene un número de aislamiento menor en comparación con un punto no atípico. Por lo tanto, un punto de datos se define como valor atípico si su número de aislamiento es menor que el umbral. El umbral se define en función del porcentaje estimado de valores atípicos en los datos, que es el punto de partida de este algoritmo de detección de valores atípicos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import IsolationForest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar datos normales\n",
    "rng = np.random.RandomState(42)\n",
    "X = 0.3 * rng.randn(100, 2)\n",
    "X = np.r_[X + 2, X - 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar outliers\n",
    "X_outliers = rng.uniform(low=-4, high=4, size=(20, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unir datos\n",
    "X_total = np.r_[X, X_outliers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustar Isolation Forest\n",
    "clf = IsolationForest(contamination=0.1, random_state=42) #El parámetro `contamination` indica la proporción esperada de outliers\n",
    "y_pred = clf.fit_predict(X_total)\n",
    "\n",
    "# Visualización\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(X_total[:, 0], X_total[:, 1], c=(y_pred==-1), cmap='coolwarm', s=40, edgecolors='k')\n",
    "plt.title('Detección de outliers con Isolation Forest')\n",
    "plt.xlabel('X1')\n",
    "plt.ylabel('X2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ejemplo dataser california housing\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "data = fetch_california_housing(as_frame=True)\n",
    "df = data.frame\n",
    "\n",
    "# Seleccionar dos variables para visualizar\n",
    "X_real = df[['AveRooms', 'AveOccup']].values\n",
    "\n",
    "# Ajustar Isolation Forest\n",
    "clf_real = IsolationForest(contamination=0.05, random_state=0)\n",
    "y_pred_real = clf_real.fit_predict(X_real)\n",
    "\n",
    "# Visualización\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(X_real[:,0], X_real[:,1], c=(y_pred_real==-1), cmap='coolwarm', s=10, alpha=0.5)\n",
    "plt.xlabel('AveRooms')\n",
    "plt.ylabel('AveOccup')\n",
    "plt.title('Outliers en California Housing (Isolation Forest)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBSCAN\n",
    "\n",
    "DBSCAN (Density-Based Spatial Clustering of Applications with Noise) es un algoritmo de clustering basado en densidad, agrupa puntos que están densamente conectados y considera como outliers (ruido) aquellos puntos que no pertenecen a ningún grupo. Es como K-means, excepto que no es necesario especificar el número de grupos de antemano.\n",
    "\n",
    "Parámetros principales:\n",
    "- `eps`: Radio de vecindad para considerar puntos vecinos.\n",
    "- `min_samples`: Número mínimo de puntos para formar un cluster (incluyendo el propio punto)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# Generar datos de dos clusters\n",
    "rng = np.random.RandomState(0)\n",
    "X1 = rng.normal(loc=[2, 2], scale=0.5, size=(50, 2))\n",
    "X2 = rng.normal(loc=[-2, -2], scale=0.5, size=(50, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar outliers\n",
    "X_outliers = rng.uniform(low=-6, high=6, size=(10, 2))\n",
    "\n",
    "# Unir datos\n",
    "X = np.vstack([X1, X2, X_outliers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar DBSCAN\n",
    "db = DBSCAN(eps=0.7, min_samples=5)\n",
    "labels = db.fit_predict(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualización\n",
    "plt.figure(figsize=(8,6))\n",
    "unique_labels = set(labels)\n",
    "colors = [plt.cm.tab10(each) for each in np.linspace(0, 1, len(unique_labels))]\n",
    "for k, col in zip(unique_labels, colors):\n",
    "    if k == -1:\n",
    "        # Outliers\n",
    "        col = (1, 0, 0, 1)\n",
    "        label = 'Outlier'\n",
    "    else:\n",
    "        label = f'Cluster {k}'\n",
    "    class_member_mask = (labels == k)\n",
    "    plt.plot(X[class_member_mask, 0], X[class_member_mask, 1], 'o', markerfacecolor=col, markeredgecolor='k', markersize=10, label=label)\n",
    "plt.title('DBSCAN: clusters y outliers')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo dataset insurance\n",
    "df_3 = pd.read_csv(\"Data/insurance.csv\")\n",
    "df_3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "X = df_3[['age','bmi']].values\n",
    "\n",
    "db = DBSCAN(eps=3.0, min_samples=10).fit(X)\n",
    "labels = db.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(labels).value_counts() #-1 representan valores atipicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels = set(labels)\n",
    "colors = ['blue', 'red']\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "for color,label in zip(colors, unique_labels):\n",
    "    sample_mask = [True if l == label else False for l in labels]\n",
    "    plt.plot(X[:,0][sample_mask], X[:, 1][sample_mask], '*', color=color);\n",
    "plt.xlabel('Age');\n",
    "plt.ylabel('BMI');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## ¿Qué hacer después de identificar los outliers?\n",
    "\n",
    "> **Mantenerlos:** Podemos mantener los outliers si consideramos que pueden ser representativos de un subconjunto de nuestros datos.\n",
    "\n",
    "> **Eliminarlos:** Si estamos seguros de que los outliers provienen de un error en la entrada de los datos, como un error humano o de medida, y no podemos solucionarlo, podemos eliminarlos del conjunto de datos.\n",
    "\n",
    "> **Imputar:** La imputación implica reemplazar los valores atípicos con otros valores como la mediana o la media. Esto se suele hacer cuando queremos conservar la mayor cantidad de datos, pero eliminando el efecto de los outliers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yout = y.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(x,y,'xr', label='datos')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin2 = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin2.fit(xi, yout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin2.intercept_[0], lin2.coef_[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "betaout = [lin2.intercept_[0], lin2.coef_[0][0]]\n",
    "betaout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yout = b0 + b1*x\n",
    "y_fit = beta[0] + beta[1]*x\n",
    "y_fitout = betaout[0] + betaout[1]*x\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(x,y,'xr', label='datos')\n",
    "plt.plot(x,y_fit, 'b', lw=3)\n",
    "plt.plot(x,y_fitout, 'g', lw=3)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOF+MzbeIqhDgOLnPBA//FT",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
