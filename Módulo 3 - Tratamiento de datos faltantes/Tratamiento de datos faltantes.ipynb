{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: left;;\" src='Figures/iteso.jpg' width=\"100\" height=\"200\"/></a>\n",
    "\n",
    "# <center> <font color= #000047> Tratamiento de datos Faltantes </font> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El manejo de datos faltantes es un aspecto fundamental en el análisis de datos. Comprender la naturaleza de los datos faltantes permite seleccionar el método de imputación más adecuado y evitar sesgos en los resultados.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualización de Datos Faltantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 436
    },
    "executionInfo": {
     "elapsed": 1695,
     "status": "ok",
     "timestamp": 1676423709019,
     "user": {
      "displayName": "Carlos Arellano",
      "userId": "03055712073717640513"
     },
     "user_tz": 360
    },
    "id": "fz8HGKryK00p",
    "outputId": "b1078bb1-3900-4e7a-88bd-9e11494e8711"
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv('API_SI.POV.DDAY_DS2.csv',encoding='latin-1',sep='\\t')\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1676423709019,
     "user": {
      "displayName": "Carlos Arellano",
      "userId": "03055712073717640513"
     },
     "user_tz": 360
    },
    "id": "8gydVlbSLSkg",
    "outputId": "6630dbbe-211b-410f-b9ce-1e984f9604ee"
   },
   "outputs": [],
   "source": [
    "# Mapa de calor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 883
    },
    "executionInfo": {
     "elapsed": 347,
     "status": "ok",
     "timestamp": 1676423709362,
     "user": {
      "displayName": "Carlos Arellano",
      "userId": "03055712073717640513"
     },
     "user_tz": 360
    },
    "id": "IHCEc5X28cVY",
    "outputId": "08c8a869-a688-4135-d45d-ffd7e19e33b0",
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Librería para visualización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalación de missingno si es necesario\n",
    "#!pip install missingno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualización de datos faltantes con missingno\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bar plot \n",
    "#color='steelblue')\n",
    "#color=\"tomato\")\n",
    "#color=\"tab:green\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación de Datos Faltantes\n",
    "\n",
    "Rubin (1976) propuso una clasificación ampliamente aceptada de los mecanismos de datos faltantes:\n",
    "\n",
    "> **MCAR (Missing Completely At Random)**: Los datos faltantes ocurren completamente al azar y no dependen ni de los valores observados ni de los no observados.\n",
    "\n",
    "> **MAR (Missing At Random)**: La probabilidad de que un dato esté ausente depende solo de los valores observados, no de los valores faltantes.\n",
    "\n",
    "> **MNAR (Missing Not At Random)**: La probabilidad de que un dato esté ausente depende de los valores faltantes en sí mismos, incluso después de considerar los valores observados.\n",
    " \n",
    "<img src=\"Figures/tipo_faltante.png\" width=\"600\" height=\"600\">\n",
    "\n",
    "[https://towardsdatascience-com.translate.goog/missing-value-imputation-explained-a-visual-guide-with-code-examples-for-beginners-93e0726284eb/?_x_tr_sl=en&_x_tr_tl=es&_x_tr_hl=es&_x_tr_pto=tc]('liga')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MCAR (Missing Completely at Random)\n",
    "MCAR es el más sencillo de los tres. Un conjunto de datos es MCAR si la probabilidad de que falte un dato es igual para todos los individuos y no depende de las medidas de otras variables, es decir, no existe ninguna relación entre que un dato sea faltante u observado.\n",
    "\n",
    "**Ejemplo**:\n",
    "- Partiendo de una tabla de una base de datos, si ocurre un problema informático y se pierden algunos valores de algunas observaciones de forma aleatoria tendríamos pérdida MCAR.\n",
    "\n",
    "- En una encuesta, algunas personas omiten una pregunta por accidente, sin relación con sus características o respuestas.\n",
    "\n",
    "- [Iris (UCI Machine Learning Repository)](https://archive.ics.uci.edu/ml/datasets/iris). Si se eliminan filas aleatoriamente o se simulan valores faltantes en cualquier columna sin relación con las variables, los datos faltantes serían MCAR.\n",
    "\n",
    "- [Wine Quality (UCI)](https://archive.ics.uci.edu/ml/datasets/wine+quality). Si durante la medición de la calidad del vino, algunos sensores fallan aleatoriamente y se pierden datos de variables químicas sin relación con otras variables o con la calidad.\n",
    "\n",
    "Para comprobar si un conjunto de datos tiene una pérdida MCAR podemos emplear el [test de Litle](https://bpb-us-w2.wpmucdn.com/blog.nus.edu.sg/dist/4/6502/files/2018/06/mcartest-zlxtj7.pdf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo MCAR: Simulación\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(42)\n",
    "data = pd.DataFrame({\n",
    "    'edad': np.random.randint(20, 60, 100),\n",
    "    'ingreso': np.random.randint(20000, 80000, 100)\n",
    "})\n",
    "# Introducimos valores faltantes aleatoriamente (MCAR)\n",
    "mask = np.random.rand(*data.shape) < 0.1\n",
    "data_mcar = data.mask(mask)\n",
    "print('Datos con valores faltantes MCAR:')\n",
    "print(data_mcar.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prueba de Little para MCAR (Little's MCAR Test)\n",
    "\n",
    "La prueba de Little (Little's MCAR test) es un método estadístico para evaluar si los datos faltantes son MCAR (Missing Completely At Random). Si el valor p es alto (por ejemplo, > 0.05), no se rechaza la hipótesis nula de que los datos son MCAR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAR (Missing At Random)\n",
    "\n",
    "Un conjunto de datos tiene pérdida MAR cuando la probabilidad de que una varibale tenga datos faltantes es independiente de los valores de la misma variable, pero dependiente de los valores de otras variables presentes en el conjunto de datos. La probabilidad de que un dato esté ausente depende solo de los valores observados, no de los valores faltantes\n",
    "\n",
    "**Ejemplo**\n",
    "- Partiendo de un estudio en el que tenemos las variables puesto de trabajo y sueldo, estaríamos con una pérdida MAR si la gente con determinados puestos de trabajo fuera más reacia a contestar a la pregunta del sueldo.\n",
    "  \n",
    "- En un estudio médico, las personas mayores tienden a omitir preguntas sobre ingresos, pero dentro de cada grupo de edad, la omisión es aleatoria.\n",
    "\n",
    "- [Medical Expenditure Panel Survey (MEPS)](https://meps.ahrq.gov/mepsweb/). La variable 'gastos médicos' puede faltar más frecuentemente en personas jóvenes, pero dentro de cada grupo de edad, la omisión es aleatoria.\n",
    "\n",
    "- [Titanic (Kaggle)](https://www.kaggle.com/c/titanic/data). En el dataset Titanic, la variable 'Age' tiene valores faltantes. La probabilidad de que falte la edad depende de otras variables observadas como 'Pclass' o 'Sex'. Por ejemplo, es más probable que falte la edad en pasajeros de tercera clase.\n",
    "\n",
    "Si los datos son MAR, se pueden usar métodos de imputación más sofisticados (como imputación múltiple) para obtener estimaciones no sesgadas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo MAR: Simulación\n",
    "data_mar = data.copy()\n",
    "# Si la edad es mayor a 50, hay más probabilidad de que ingreso sea NaN\n",
    "prob = np.where(data_mar['edad'] > 50, 0.4, 0.05)\n",
    "mask = np.random.rand(len(data_mar)) < prob\n",
    "data_mar.loc[mask, 'ingreso'] = np.nan\n",
    "print('Datos con valores faltantes MAR:')\n",
    "print(data_mar.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  MNAR (missing not at random)\n",
    "Tenemos el caso de MNAR cuando la probabilidad de que una observación sea dato faltante es dependiente del valor de la propia variable. Su efecto no se puede ignorar ya que el valor faltante está relacionado con la razón por la que falta el dato.\n",
    "\n",
    "**Ejemplo**\n",
    "\n",
    "- En un estudio acerca de malos hábitos alimenticios, si algunas personas con esos malos hábitos fueran menos propensas a contestar las preguntas.\n",
    "- En una encuesta de ingresos, las personas con ingresos muy altos tienden a no responder la pregunta sobre ingresos.\n",
    "- [NHANES (National Health and Nutrition Examination Survey)](https://wwwn.cdc.gov/nchs/nhanes/). En encuestas de salud, las personas con mayor peso pueden ser menos propensas a reportar su peso. Así, los valores faltantes en la variable 'peso' dependen del propio peso (no observado).\n",
    "- [European Social Survey (ESS)](https://www.europeansocialsurvey.org/). Preguntas sensibles como 'uso de drogas' o 'salud mental' pueden tener faltantes porque quienes tienen valores extremos tienden a no responder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo MNAR: Simulación\n",
    "data_mnar = data.copy()\n",
    "# Mayor probabilidad de ser NaN si el ingreso es alto\n",
    "prob = (data_mnar['ingreso'] > 60000).astype(float) * 0.5 + 0.05\n",
    "mask = np.random.rand(len(data_mnar)) < prob\n",
    "data_mnar.loc[mask, 'ingreso'] = np.nan\n",
    "print('Datos con valores faltantes MNAR:')\n",
    "print(data_mnar.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resumen Comparativo\n",
    "\n",
    "| Tipo  | ¿Depende de valores observados? | ¿Depende de valores faltantes? | Ejemplo típico |\n",
    "|-------|:------------------------------:|:-----------------------------:|:--------------:|\n",
    "| MCAR  | No                             | No                            | Omisión accidental |\n",
    "| MAR   | Sí                             | No                            | Omisión por grupo observado |\n",
    "| MNAR  | Sí/No                          | Sí                            | Omisión por valor oculto |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Métodos básicos para tratamiento de datos faltantes\n",
    "\n",
    "## Análisis con datos completos (listwise)\n",
    "Es un método muy habitual y sencillo de utilizar. Consiste en eliminar todas las observaciones que contengan algún valor faltante en alguna variable del conjunto de datos, es decir, para realizar el análisis estadístico solo se usarían las observaciones que disponen de todos los valores.\n",
    "\n",
    "**Ejemplo:** \n",
    "Partamos de un dataset $X$ con $k = 3$ variables y $n = 100$ observaciones. Una variable tendrá valores faltantes $(p = 1)$ en $m = 10$ observaciones. Si realizamos un análisis con datos completos estaríamos eliminando los datos de las m observaciones con valores faltantes y nuestro dataset pasaría de $n = 100$ observaciones a $n − m = 90$ observaciones.\n",
    "\n",
    "- Si la pérdida del conjunto de datos es MCAR, los resultados del análisis serán insesgados pues se trataría de una muestra aleatoria de los datos. La desventaja es que no es habitual la presencia de una pérdida MCAR y el análisis sería insesgado en el resto de los casos. \n",
    "\n",
    "- Otra desventaja es que se puede perder mucha información, sobre todo si se tienen muchas variables con valores faltantes. Siguiendo con el ejemplo anterior, si tuviéramos un 10% de datos faltantes en cada una de las 3 variables podríamos llegar a eliminar hasta el 30% de las observaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfcopy.head() #dataset indicando valores faltantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#valores faltantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir un threshold de valores faltantes a eliminar por registros y variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Visualización de datos faltantes después de la eliminación\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eliminación de datos por registros y variables\n",
    "# Simulando un Dataset con valores faltantes\n",
    "df_data=pd.DataFrame(np.random.randn(100,4)+10*np.random.rand(4),columns=['A','B','C','D'])\n",
    "for c in df_data.columns[:-1]:\n",
    "    inan=np.random.randint(100,size=np.random.randint(20))\n",
    "    df_data.loc[inan, c]=np.NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminación de filas (Observaciones)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminación de columnas (Variables)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Métodos de Imputación\n",
    "\n",
    "Otra forma de tratar los valores faltantes es imputar el valor faltante por un valor. Se usa la información presente en los valores observados para establecer un valor en aquellos valores no observados. Es importante saber elegir bien el método de imputación ya que cada uno tiene sus ventajas e inconvenientes.\n",
    "\n",
    "- A la hora de hacer la imputación es importante mantener la consistencia de los datos. También es necesario mantener las distribuciones de las variables, así como sus correlaciones para evitar una distorsión de los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputación por un métricas centrales de posición\n",
    "\n",
    "#### Imputación por la media\n",
    "Sustituye los valores faltantes de cada variable por la media muestral de la propia variable.\n",
    "\n",
    "- Este método funciona bien bajo el supuesto de datos MCAR.\n",
    "\n",
    "-  No funciona bien cuando los valores faltantes dependen de otra variable. Esto es porque si se sustituyen los datos faltantes por la media, estaríamos reduciendo la varianza de cada variable y, por ende, también se modifcarían las matrices de covarianza y de correlaciones.\n",
    "\n",
    "- Otra desventaja de este procedimiento es que tan solo es aplicable a variables cuantitativas y no a a variables cualitativas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un DataFrame de ejemplo\n",
    "data = {\n",
    "    'A': [1, 2, None, 4],\n",
    "    'B': [5, None, None, 8],\n",
    "    'C': [9, 10, 11, 12]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "print('DataFrame original:')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputar con la media\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Imputar con la media\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ver las distribuciones\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputación por la mediana\n",
    "\n",
    "Se trata de un procedimiento similar al de la media, con la diferencia de que se sustituyen los valores de cada variable por la mediana de la propia variable. \n",
    "\n",
    "- **ventaja:** la imputación por la mediana es más robusta a la aparición de datos atípicos. Al igual que la imputación por la media funciona bien bajo el supuesto MCAR.\n",
    "- Si no es MCAR, estaríamos reduciendo mucho la varianza de cada variable y modifcando las matrices de covarianza y correlaciones. También es aplicable sólo a variables cuantitativas.\n",
    "- No recomendable si los datos están sesgados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputar con la mediana\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputar con la mediana con la librería\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ver dist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputación por la moda\n",
    "\n",
    "Procedimiento muy similar a los dos descritos anteriormente, con la diferencia de que se imputa con el valor más frecuente o moda.\n",
    "\n",
    "- Se puede usar para imputar las variables cualitativas o categóricas. Por el contrario, no es recomendable usarlo para imputar variables cuantitativas pues la media o mediana son una mejor representación de este tipo de variables. \n",
    "- **Desventaja:** Puede reducir la varianza de los datos, no considera la relación entre variables, no recomendable si los datos están sesgados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputar con la moda\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputar con la moda\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# otro ejemplo titanic\n",
    "# Seleccionar columnas categóricas con valores faltantes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputación Adelante (Forward Fill) y Atrás (Backward Fill)\n",
    "\n",
    "Consiste en rellenar los valores faltantes con el valor anterior (forward fill) o posterior (backward fill). Efectivos para rellenar valores faltantes en series temporales o datos ordenados.\n",
    "\n",
    "##### Imputación Adelante (Forward Fill)\n",
    "\n",
    "Consiste en reemplazar cada valor faltante con el último valor observado previamente en la serie.\n",
    "\n",
    "Formalmente, para una serie $x_1, x_2, \\ldots, x_n$ con valores faltantes, la imputación adelante se define como:\n",
    "\n",
    "$$\n",
    "x_t = \\begin{cases}\n",
    "    x_t, & \\text{si } x_t \\text{ no es faltante} \\\\\n",
    "    x_{t-1}^{*}, & \\text{si } x_t \\text{ es faltante}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "donde $x_{t-1}^{*}$ es el valor imputado más reciente antes de $t$.\n",
    "\n",
    "##### Imputación Atrás (Backward Fill)\n",
    "\n",
    "Consiste en reemplazar cada valor faltante con el siguiente valor observado en la serie.\n",
    "\n",
    "$$\n",
    "x_t = \\begin{cases}\n",
    "    x_t, & \\text{si } x_t \\text{ no es faltante} \\\\\n",
    "    x_{t+1}^{*}, & \\text{si } x_t \\text{ es faltante}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "donde $x_{t+1}^{*}$ es el siguiente valor observado después de $t$.\n",
    "\n",
    "**Desventajas:**\n",
    "- Puede propagar errores si hay secuencias largas de valores faltantes.\n",
    "- No recomendable para datos no ordenados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear una serie temporal con valores faltantes\n",
    "np.random.seed(0)\n",
    "dates = pd.date_range('2023-01-01', periods=20)\n",
    "values = np.random.randn(20).cumsum()\n",
    "series = pd.Series(values, index=dates)\n",
    "\n",
    "# Introducir valores faltantes\n",
    "series.iloc[[3, 4, 10, 11, 12, 17]] = np.nan\n",
    "print('Serie original con valores faltantes:')\n",
    "print(series)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputación adelante (forward fill)\n",
    "\n",
    "# Imputación atrás (backward fill)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualización\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(series, 'o-', label='Original')\n",
    "plt.plot(series_ffill, 's--', label='Forward Fill')\n",
    "plt.plot(series_bfill, 'd--', label='Backward Fill')\n",
    "plt.xticks(rotation=30, ha='right')\n",
    "plt.legend()\n",
    "plt.title('Imputación Forward Fill y Backward Fill')\n",
    "plt.xlabel('Fecha')\n",
    "plt.ylabel('Valor')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Es recomendable usar estos métodos solo cuando los datos tienen un orden natural (por ejemplo, tiempo).\n",
    "- Si los valores faltantes están al inicio o final de la serie, forward fill o backward fill pueden no imputar todos los valores.\n",
    "- Se pueden combinar ambos métodos para imputar valores al inicio y final:\n",
    "\n",
    "```python\n",
    "serie_imputada = serie.ffill().bfill()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputación Hot Deck y Cold Deck\n",
    "\n",
    "La imputación Hot Deck y Cold Deck son técnicas utilizadas para rellenar valores faltantes especialmente en encuestas y estudios sociales. Ambas se basan en reemplazar valores faltantes con valores observados de otros registros, pero difieren en la fuente de los datos donantes.\n",
    "\n",
    "#### Imputación Hot Deck\n",
    "\n",
    "Consiste en reemplazar cada valor faltante con un valor observado de otro registro dentro del mismo conjunto de datos (el \"deck caliente\"). El donante puede seleccionarse aleatoriamente, por proximidad, o por pertenencia a un grupo similar.\n",
    "\n",
    "$$x_{i,\\text{imputado}} = x_{j,\\text{observado}}, \\quad j \\in \\mathcal{D}, \\quad j \\neq i$$\n",
    "donde $\\mathcal{D}$ es el conjunto de donantes válidos. Usando este método tenemos la ventaja de que sirve tanto para variables categóricas como numéricas, también que se preserva la distribución de cada variable.\n",
    "\n",
    "#### Imputación Cold Deck\n",
    "\n",
    "Similar a Hot Deck, pero los valores donantes provienen de un conjunto de datos externo o de una fuente histórica (el \"deck frío\").\n",
    "\n",
    "$$x_{i,\\text{imputado}} = x_{k,\\text{externo}}, \\quad k \\in \\mathcal{D}_{\\text{externo}}$$\n",
    "\n",
    "donde $\\mathcal{D}_{\\text{externo}}$ es el conjunto de donantes del dataset externo.\n",
    "\n",
    "**Hot Deck:** Mantiene la coherencia interna del dataset. Puede preservar la distribución y relaciones entre variables. Puede ser aleatorio o basado en similitud.\n",
    "\n",
    "**Cold Deck:** Útil cuando se dispone de fuentes externas confiables. Puede introducir sesgos si los datos externos no son comparables.\n",
    "\n",
    "**Desventajas:**\n",
    "- Puede ser sensible a la selección de donantes.\n",
    "- Podría incurrir en sesgos en las estimaciones de las correlaciones de las variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulación de datos\n",
    "np.random.seed(42)\n",
    "df = pd.DataFrame({\n",
    "    'edad': np.random.randint(18, 65, 20),\n",
    "    'ingreso': np.random.randint(10000, 50000, 20)\n",
    "})\n",
    "# Introducir valores faltantes\n",
    "df.loc[[2, 5, 7, 12], 'ingreso'] = np.nan\n",
    "print('Datos originales con valores faltantes:')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputación Hot Deck aleatoria\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grupo edad\n",
    "df['grupo_edad'] = pd.cut(df['edad'], bins=[17, 30, 45, 65], labels=['Joven', 'Adulto', 'Mayor'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introducimos de nuevo valores faltantes para el ejemplo\n",
    "df.loc[[2, 5, 7, 12], 'ingreso'] = np.nan\n",
    "df_imp_hotdeck_group = df.copy()\n",
    "df_imp_hotdeck_group.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputación Hot Deck por grupo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imputación Cold Deck usando un Dataset Externo: Supongamos que tenemos un dataset histórico con la misma estructura y lo usamos como fuente de donantes\n",
    "#Dataset externo (histórico)\n",
    "df_ext = pd.DataFrame({\n",
    "    'edad': np.random.randint(18, 65, 20),\n",
    "    'ingreso': np.random.randint(12000, 48000, 20)\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputación Cold Deck: tomar valores de ingreso del dataset externo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputación por Modelos Predictivos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputación mediante un modelo de regresión o estimación de media condicional\n",
    "\n",
    "Este método consiste en realizar un modelo de regresión para imputar los valores faltantes con los valores predichos por el modelo de regresión.\n",
    "\n",
    "Sea $Y$ una variable con valores faltantes, la imputación por regresión se basa en ajustar un modelo de la forma:\n",
    "\n",
    "\n",
    "$$Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_p X_p + \\varepsilon$$\n",
    "\n",
    "Donde:\n",
    "- $Y$ es la variable con valores faltantes.\n",
    "- $X_1, X_2, \\dots, X_p$ son las variables predictoras.\n",
    "- $\\beta_0, \\beta_1, \\dots, \\beta_p$ son los coeficientes estimados.\n",
    "- $\\varepsilon$ es el término de error.\n",
    "\n",
    "Para imputar, se predice $\\hat{Y}$ usando los valores observados de $X$ en las filas con $Y$ faltante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Supongamos un dataset con varias variables predictoras. Imputaremos valores faltantes en 'y' usando regresión múltiple.\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Simulación de datos con múltiples variables\n",
    "np.random.seed(42)\n",
    "n = 150\n",
    "X1 = np.random.normal(5, 2, n)\n",
    "X2 = np.random.normal(10, 3, n)\n",
    "X3 = np.random.normal(20, 5, n)\n",
    "y = 3 + 2*X1 - 1.5*X2 + 0.5*X3 + np.random.normal(0, 2, n)\n",
    "df_multi = pd.DataFrame({'X1': X1, 'X2': X2, 'X3': X3, 'y': y})\n",
    "df_multi.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introducimos valores faltantes en 'y' (simulando valores faltantes)\n",
    "mask = np.random.rand(n) < 0.18\n",
    "df_multi.loc[mask, 'y'] = np.nan\n",
    "df_multi.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separar datos completos e incompletos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustar regresión múltiple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputar valores faltantes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Desventajas:**\n",
    "- Si se usan modelos paramétricos de regresión se pueden llegar a producir estimaciones sesgadas. Por ejemplo, si se usa un modelo lineal los valores imputados caerán en una línea recta o en el hiperplano, dependiendo de la cantidad de dimensiones.\n",
    "- La correlación entre los datos imputados es igual a 1, por lo que las correlaciones estarían sobreestimadas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una solución puede ser añadir un resíduo aleatorio a cada valor imputado (**regresión estocástica**). Así conseguimos mantener las correlaciones entre las variables y se podría reducir el sesgo de la imputación por regresión. El residuo aleatorio se suele generar a partir de una distribución normal.\n",
    "\n",
    "La imputación por **regresión estocástica** añade un término aleatorio al valor imputado, preservando la varianza original de los datos. La ecuación es:\n",
    "\n",
    "$$\\hat{Y}_i = \\beta_0 + \\beta_1 X_{i1} + \\dots + \\beta_p X_{ip} + \\hat{\\varepsilon}_i$$\n",
    "\n",
    "donde $\\hat{\\varepsilon}_i$ es un valor aleatorio muestreado del residuo del modelo ajustado.\n",
    "\n",
    "Esto evita que los valores imputados sean demasiado \"perfectos\" y subestimen la variabilidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulación de datos\n",
    "np.random.seed(123)\n",
    "n = 120\n",
    "X1 = np.random.normal(8, 2, n)\n",
    "X2 = np.random.normal(15, 4, n)\n",
    "y = 5 + 1.2*X1 - 0.8*X2 + np.random.normal(0, 3, n)\n",
    "df_stoch = pd.DataFrame({'X1': X1, 'X2': X2, 'y': y})\n",
    "\n",
    "# Introducimos valores faltantes en 'y'\n",
    "mask = np.random.rand(n) < 0.2\n",
    "df_stoch.loc[mask, 'y'] = np.nan\n",
    "df_stoch.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separar datos completos e incompletos\n",
    "df_complete = df_stoch[df_stoch['y'].notnull()]\n",
    "df_missing = df_stoch[df_stoch['y'].isnull()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustar modelo de regresión\n",
    "model = LinearRegression()\n",
    "model.fit(df_complete[['X1', 'X2']], df_complete['y'])\n",
    "\n",
    "# Calcular residuos del modelo\n",
    "residuals = df_complete['y'] - model.predict(df_complete[['X1', 'X2']])\n",
    "residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputar valores faltantes añadiendo ruido aleatorio (regresión estocástica)\n",
    "imputed_values = model.predict(df_missing[['X1', 'X2']]) + np.random.choice(residuals, size=len(df_missing))\n",
    "df_stoch.loc[df_stoch['y'].isnull(), 'y'] = imputed_values\n",
    "df_stoch.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Imputación de Valores Faltantes por KNN (K-Nearest Neighbors)\n",
    "\n",
    "El método $KNN$ es un modelo predictivo cuyas aplicaciones incluyen la imputación de datos, ya que es un clasificador de aprendizaje supervisado no paramétrico, que utiliza la proximidad para hacer clasificaciones o predicciones sobre la agrupación de un punto de datos individual. En otras palabras, **se estima el valor perdido como la media (en el caso de las variables numéricas) de los valores de los $k$ vecinos u observaciones más cercanos. Así mismo, para las variables categóricas, se utiliza la clase mayoritaria de entre los k más cercanos.**\n",
    "\n",
    "El valor de $k$ define cuántos vecinos se verificarán para determinar la clasificación del dato faltante, siendo k directamente proporcional a la generación de sesgo e inversamente proporcional a la varianza. En general se recomienda tener un número impar de k para evitar empates en la clasificación.\n",
    "\n",
    "Para cada valor faltante, el algoritmo:\n",
    "1. Calcula la distancia entre la observación incompleta y todas las observaciones completas (usualmente distancia euclidiana).\n",
    "2. Selecciona los k vecinos más cercanos.\n",
    "3. Imputa el valor faltante usando la media (para variables numéricas) o la moda (para categóricas) de los vecinos.\n",
    "\n",
    "\n",
    "**Distancia Euclidiana:**\n",
    "$$\n",
    "d(x, y) = \\sqrt{\\sum_{i=1}^n (x_i - y_i)^2}\n",
    "$$\n",
    "\n",
    "**Imputación numérica:**\n",
    "$$\n",
    "\\hat{x}_{\\text{miss}} = \\frac{1}{k} \\sum_{j=1}^k x_{j, \\text{vecino}}\n",
    "$$\n",
    "\n",
    "**Imputación categórica:**\n",
    "- Se utiliza la moda de los vecinos.\n",
    "\n",
    "Algunas ventajas de este método son: aprovecha la similitud entre observaciones y puede preservar relaciones complejas entre variables.\n",
    "\n",
    "**Desventajas:** Computacionalmente costoso para grandes datasets. Sensible a la escala de las variables (es recomendable normalizar antes de imputar). El valor de $k$ debe seleccionarse cuidadosamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imputación para variables cuantitativas\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import KNNImputer\n",
    "# Cargar datos\n",
    "iris = load_iris(as_frame=True)\n",
    "df_iris = iris.data.copy()\n",
    "\n",
    "# Simular valores faltantes\n",
    "np.random.seed(1)\n",
    "mask = np.random.rand(*df_iris.shape) < 0.1\n",
    "df_iris[mask] = np.nan\n",
    "\n",
    "# Normalizar antes de imputar\n",
    "scaler = StandardScaler()\n",
    "df_scaled = pd.DataFrame(scaler.fit_transform(df_iris), columns=df_iris.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputación KNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imputación para variables Categóricas\n",
    "# Para variables categóricas, se recomienda codificar las categorías numéricamente antes de imputar y luego decodificar tras la imputación.\n",
    "df_cat = pd.DataFrame({\n",
    "    'color': ['rojo', 'azul', 'verde', np.nan, 'azul', 'rojo', np.nan],\n",
    "    'forma': ['circulo', 'cuadro', 'triangulo', 'cuadro', np.nan, 'circulo', 'triangulo']\n",
    "})\n",
    "df_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codificar categorías\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputar con KNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decodificar\n",
    "for col in df_cat.columns:\n",
    "    cats = ['azul', 'circulo', 'cuadro', 'rojo', 'triangulo', 'verde']\n",
    "    # Ajustar categorías según columna\n",
    "    if col == 'color':\n",
    "        mapping = {0: 'azul', 1: 'rojo', 2: 'verde'}\n",
    "    else:\n",
    "        mapping = {0: 'circulo', 1: 'cuadro', 2: 'triangulo'}\n",
    "    df_cat_imputed[col] = df_cat_imputed[col].round().astype(int).map(mapping)\n",
    "\n",
    "print('Datos categóricos tras imputación KNN:')\n",
    "df_cat_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputación de Valores Faltantes con Missing Forest (MissForest)\n",
    "\n",
    "La imputación por \"Missing Forest\" (MissForest) es un método basado en bosques aleatorios (Random Forests) para imputar valores faltantes en datasets tanto numéricos como categóricos. Es un método no paramétrico y robusto que puede capturar relaciones no lineales y complejas entre variables.\n",
    "\n",
    "**MissForest** utiliza un enfoque iterativo:\n",
    "1. Inicializa los valores faltantes (por ejemplo, con la media o moda).\n",
    "2. Para cada variable con valores faltantes, entrena un Random Forest usando las otras variables como predictores.\n",
    "3. Imputa los valores faltantes usando las predicciones del modelo.\n",
    "4. Repite el proceso para todas las variables con valores faltantes hasta que la imputación converge o se alcanza un número máximo de iteraciones.\n",
    "\n",
    "Para una variable $X_j$ con valores faltantes:\n",
    "\n",
    "$$X_{j,\\text{miss}} = f_{RF}(X_{-j,\\text{obs}})$$\n",
    "\n",
    "donde $f_{RF}$ es el modelo de Random Forest ajustado usando las otras variables $X_{-j}$ como predictores y los valores observados de $X_j$ como objetivo.\n",
    "\n",
    "El proceso se repite para cada variable con valores faltantes y se actualizan las imputaciones en cada iteración.\n",
    "\n",
    "Puede manejar variables numéricas y categóricas. Captura relaciones no lineales y complejas. No requiere supuestos de distribución.\n",
    "\n",
    "**Desventajas:** Computacionalmente intensivo para grandes datasets. Puede sobreajustar si hay pocos datos o muchas variables irrelevantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputación tipo MissForest manual para variables numéricas usando scikit-learn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Simulación de datos\n",
    "np.random.seed(0)\n",
    "df = pd.DataFrame({\n",
    "    'A': np.random.normal(10, 2, 20),\n",
    "    'B': np.random.normal(5, 1, 20)\n",
    "})\n",
    "df.loc[[2, 5, 7], 'A'] = np.nan\n",
    "df.loc[[1, 6, 12], 'B'] = np.nan\n",
    "\n",
    "print('Datos originales con valores faltantes:')\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputación iterativa tipo MissForest (solo numérico, ejemplo básico)\n",
    "df_imputed = df.copy()\n",
    "for col in df.columns:\n",
    "    mask = df_imputed[col].isnull()\n",
    "    if mask.any():\n",
    "        train = df_imputed.loc[~mask]\n",
    "        test = df_imputed.loc[mask]\n",
    "        X_train = train.drop(columns=[col])\n",
    "        y_train = train[col]\n",
    "        X_test = test.drop(columns=[col])\n",
    "        # Imputar valores faltantes en predictores con la media temporalmente\n",
    "        X_train = X_train.fillna(X_train.mean())\n",
    "        X_test = X_test.fillna(X_train.mean())\n",
    "        rf = RandomForestRegressor(n_estimators=100, random_state=0)\n",
    "        rf.fit(X_train, y_train)\n",
    "        df_imputed.loc[mask, col] = rf.predict(X_test)\n",
    "\n",
    "print('\\nDatos tras imputación tipo MissForest manual:')\n",
    "df_imputed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputación múltiple\n",
    "La imputación múltiple se caracteriza por devolver más de un valor para cada valor faltante. Cada uno de los valores faltantes se imputan m veces, obteniendo m conjuntos de datos completos. Estos múltiples valores se combinan para obtener los valores imputados. Para combinar estos valores se puede usar la media o mediana en el caso de variables numéricas, mientras que para variables categóricas podemos emplear la moda. También se podría escoger\n",
    "uno de los valores de forma aleatoria.\n",
    "\n",
    "El proceso de imputación múltiple típicamente sigue estos pasos:\n",
    "1. **Imputación:** Se generan $m$ datasets completos, imputando los valores faltantes de manera diferente en cada uno (usando métodos estocásticos).\n",
    "2. **Análisis:** Se realiza el análisis estadístico deseado en cada dataset imputado.\n",
    "3. **Combinación:** Se combinan los resultados de los $m$ análisis para obtener estimaciones finales y errores estándar ajustados.\n",
    "\n",
    "Sea $\\hat{Q}_i$ la estimación del parámetro de interés en el dataset imputado $i$ ($i=1,\\ldots,m$), y $U_i$ su varianza estimada.\n",
    "\n",
    "- **Estimación combinada:**\n",
    "$$\n",
    "\\bar{Q} = \\frac{1}{m} \\sum_{i=1}^m \\hat{Q}_i\n",
    "$$\n",
    "\n",
    "- **Varianza total:**\n",
    "$$\n",
    "T = \\bar{U} + \\left(1 + \\frac{1}{m}\\right)B\n",
    "$$\n",
    "donde:\n",
    "$$\n",
    "\\bar{U} = \\frac{1}{m} \\sum_{i=1}^m U_i \\quad \\text{y} \\quad B = \\frac{1}{m-1} \\sum_{i=1}^m (\\hat{Q}_i - \\bar{Q})^2\n",
    "$$\n",
    "\n",
    "Al generar varias imputaciones por cada valor faltante y combinarlas, estamos realizando estimaciones más precisas y menos sesgadas de los valores faltantes. Otra ventaja es su posible aplicación tanto en variables numéricas como variables categóricas. Sus desventajas podrían ser el coste computacional de realizar varias imputaciones para cada valor faltante y el hecho de elegir un criterio para combinar estos valores.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer  # noqa\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "# Simulación de datos con valores faltantes\n",
    "np.random.seed(0)\n",
    "df = pd.DataFrame({\n",
    "    'A': np.random.normal(10, 2, 20),\n",
    "    'B': np.random.normal(5, 1, 20),\n",
    "    'C': np.random.normal(0, 1, 20)\n",
    "})\n",
    "df.loc[[2, 5, 7], 'A'] = np.nan\n",
    "df.loc[[1, 6, 12], 'B'] = np.nan\n",
    "df.loc[[3, 8, 13], 'C'] = np.nan\n",
    "\n",
    "print('Datos originales con valores faltantes:')\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputación múltiple (MICE) - se puede repetir varias veces para obtener diferentes imputaciones\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular la media y varianza de 'A' en cada dataset imputado\n",
    "means = np.array([d['A'].mean() for d in imputed_datasets])\n",
    "vars_ = np.array([d['A'].var(ddof=1)/len(d) for d in imputed_datasets])  # varianza de la media"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimación combinada y varianza total\n",
    "Q_bar = means.mean()\n",
    "U_bar = vars_.mean()\n",
    "B = means.var(ddof=1)\n",
    "T = U_bar + (1 + 1/len(means)) * B\n",
    "std_error = np.sqrt(T)\n",
    "\n",
    "print(f\"Media combinada de 'A': {Q_bar:.3f}\")\n",
    "print(f\"Error estándar combinado: {std_error:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OTRO EJEMPLO\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Cargar datos\n",
    "iris = load_iris(as_frame=True)\n",
    "df_iris = iris.data.copy()\n",
    "\n",
    "# Simular valores faltantes\n",
    "np.random.seed(1)\n",
    "mask = np.random.rand(*df_iris.shape) < 0.1\n",
    "df_iris[mask] = np.nan\n",
    "df_iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputación múltiple (MICE)\n",
    "imputer = IterativeImputer(random_state=0, sample_posterior=True, max_iter=10)\n",
    "imputed_iris = []\n",
    "for i in range(5):\n",
    "    imputer.random_state = i\n",
    "    imputed = pd.DataFrame(imputer.fit_transform(df_iris), columns=df_iris.columns)\n",
    "    imputed_iris.append(imputed)\n",
    "\n",
    "print('Primeras filas de la primera imputación en Iris:')\n",
    "print(imputed_iris[0].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Otro ejemplo\n",
    "file_path = \"https://raw.githubusercontent.com/selva86/datasets/master/Churn_Modelling_m.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df.loc[:, [\"Balance\", \"Age\", \"Exited\"]] # Usamos tres características numéricas\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer.fit(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imputed = imputer.transform(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:, [\"Balance\", \"Age\", \"Exited\"]] = df_imputed\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> <font color= #000047> Tratamiento de datos Faltantes Práctica</font> </center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import missingno as msno\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.experimental import enable_iterative_imputer  # noqa\n",
    "from sklearn.impute import IterativeImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Cargar un dataset real (Titanic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Análisis exploratorio inicial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionar variables relevantes para imputación (numéricas y categóricas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar distribución de variables numéricas antes de imputar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Imputación de variables numéricas\n",
    "\n",
    "# a) Imputación con la mediana (robusta a outliers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b) Imputación con KNN (considerando todas las numéricas seleccionadas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c) Imputación con IterativeImputer (MICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparar distribuciones tras imputación para 'age' y 'fare'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo 2:\n",
    "\n",
    "Diamonds Dataset, contiene información detallada sobre **más de 53,000 diamantes**, incluyendo características físicas, calidad y precio. Cada fila representa un diamante individual.\n",
    "\n",
    "Features:\n",
    "- **carat**: Peso del diamante (en quilates).\n",
    "- **cut**: Calidad del corte (Fair, Good, Very Good, Premium, Ideal).\n",
    "- **color**: Color del diamante, de J (peor) a D (mejor).\n",
    "- **clarity**: Pureza del diamante (de I1, SI2, SI1, VS2, VS1, VVS2, VVS1, IF).\n",
    "- **depth**: Profundidad total como porcentaje del ancho promedio.\n",
    "- **table**: Ancho de la parte superior del diamante como porcentaje del ancho promedio.\n",
    "- **price**: Precio en dólares estadounidenses.\n",
    "- **x**: Longitud en milímetros.\n",
    "- **y**: Ancho en milímetros.\n",
    "- **z**: Profundidad en milímetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputación con la media (numéricas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputación con la mediana (numéricas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputación con KNN (numéricas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputación con IterativeImputer (MICE, numéricas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputación forward fill y backward fill (numéricas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputación aleatoria (numéricas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualización comparativa de imputaciones para 'carat'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputación por la moda (categóricas)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No existe una forma perfecta de tratar los valores perdidos. Cada estrategia puede funcionar mejor para ciertos conjuntos de datos y tipos de datos faltantes, pero puede funcionar mucho peor en otros tipos de conjuntos de datos. \n",
    "\n",
    "Hay algunas reglas establecidas para decidir qué estrategia usar para tipos particulares de valores perdidos, pero más allá de eso, se debe experimentar y verificar qué modelo funciona mejor para su conjunto de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Bibliografía\n",
    "- Rubin, D. B. (1976). Inference and missing data. *Biometrika*, 63(3), 581-592.\n",
    "- Little, R. J. A., & Rubin, D. B. (2019). *Statistical Analysis with Missing Data* (3rd ed.). Wiley.\n",
    "- Schafer, J. L., & Graham, J. W. (2002). Missing data: our view of the state of the art. *Psychological Methods*, 7(2), 147–177.\n",
    "- Enders, C. K. (2010). *Applied Missing Data Analysis*. Guilford Press.\n",
    "- Van Buuren, S. (2018). *Flexible Imputation of Missing Data* (2nd ed.). CRC Press."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMTW0CgiNuDtQqTMAAgsXcO",
   "mount_file_id": "1eR5u1Ux1eGsjY_TD19_6pumO3HQNdvkQ",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
