{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52abda8b",
   "metadata": {},
   "source": [
    "<img style=\"float: left;;\" src='Figures/iteso.jpg' width=\"100\" height=\"200\"/></a>\n",
    "\n",
    "# <center> <font color= #000047> Arboles de decisión </font> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1a588b",
   "metadata": {},
   "source": [
    "## Arboles de Desición\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7fa6b1",
   "metadata": {},
   "source": [
    "Los árboles de decisión, también conocidos como modelos de árbol de clasificación y regresión (CART), son métodos basados en árboles para el aprendizaje automático supervisado. Los árboles de clasificación y de regresión simples son fáciles de usar e interpretar, pero no son competitivos con los mejores métodos de aprendizaje automático. Sin embargo, forman la base para el conjunto de modelos de ensamblaje como “bagged trees”, “random forest” y “boosted trees”, que aunque son menos interpretables, son muy precisos.\n",
    "\n",
    "Los modelos CART se puede definir en dos tipos de problemas\n",
    "\n",
    "**Árboles de clasificación:** la variable resultado es categórica y el métodos se utiliza para identificar la “clase” dentro de la cual es más probable que caiga nuestra variable resultado. Un ejemplo de un problema de tipo clasificación sería determinar quién se suscribirá o no a una plataforma digital; o quién se graduará o no de la escuela secundaria; o si una persona tiene cáncer o no.\n",
    "\n",
    "**Árboles de regressión:** la variable resultado es continua y el métodos se utiliza para predecir su valor. Un ejemplo de un problema de tipo regresión sería predecir los precios de venta de una casa residencial o el nivel de colesterol de una persona.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad515c2",
   "metadata": {},
   "source": [
    "Un árbol de decisión es una secuencia de operadores relacionales organizados como árbol donde:\n",
    "\n",
    "- Los atributos de un dato son evaluados desde la raiz hasta las hojas\n",
    "- Los nodos hoja (terminales) están asociados a una clase\n",
    "- Los nodos no-hoja están asociados a un operador lógico que divide los datos en dos o más conjuntos\n",
    "- El operador lógico o *split* se aplica sobre un atributo (feature) de los datos\n",
    "\n",
    "El siguiente diagrama ejemplifica el funcionamiento del árbol de decisión sobre un dataset con dos etiquetas y dos atributos (X y Z). \n",
    "\n",
    "<img src=\"Figures/tree.png\" width=\"600\">\n",
    "\n",
    "- La figura izquierda muestra un árbol de decisión binario con 5 nodos: 3 nodos hoja y 2 nodos de decisión.\n",
    "- La figura derecha muestra la partición que produce el árbol de decisión en el espacio de los datos. \n",
    "- Las separaciones o *splits* son siempre perpendiculares a los ejes de los datos (atributos).\n",
    "\n",
    "\n",
    "Entrenar el árbol de decisión es el proceso de escoger los atributos, operadores y umbrales de separación en los nodos de decisión.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d52c53b",
   "metadata": {},
   "source": [
    "La función de costo más común para los árboles de regresión es la suma de los residuos al cuadrado,\n",
    "\n",
    "$$RSS = \\sum_{k=1}^{K}\\sum_{i \\ in A_k} (y_i - \\hat{y}_{A_k})^2$$\n",
    "\n",
    "Para árboles de clasificación, es el índice de Gini,\n",
    "\n",
    "$$ G=\\sum_{c=1}^C \\hat{p}_{kc} (1-\\hat{p}_{kc})$$\n",
    "\n",
    "y la entropía (información estadística)\n",
    "\n",
    "$$ E= - \\sum_{c=1}^C \\hat{p}_{kc} log(\\hat{p}_{kc})$$\n",
    "\n",
    "dónde $\\hat{p}_{kc}$ es la proporción de observaciones de entrenamiento en el nodo $k$ que son de clase $c$. Un nodo completamente puro en un árbol binario tendría $\\hat{p} \\in \\{0,1\\}$ y $G=E=0$.  Un nodo completamente impuro en un árbol binario tendría $\\hat{p}=0.5$ y $G=0.5^2*2 = 0.25$, y $E=- (0.5 \\cdot log(0.5))\\cdot 2 = 0.69$\n",
    "\n",
    "La ganacia de información para un nodo que separa un conjunto de datos $D$ en dos $D_{izq}$ y $D_{der}$ es\n",
    "\n",
    "$$\n",
    "G(D; D_{izq}, D_{der}) = H(D) - \\frac{|D_{izq}|}{|D|} H(D_{izq}) - \\frac{|D_{der}|}{|D|} H(D_{der})\n",
    "$$\n",
    "\n",
    "donde $|A|$ es la cardinalidad del subconjunto $A$ y \n",
    "\n",
    "$$\n",
    "H(A) = - \\sum_{y \\in \\mathcal{Y}} p(y|A) \\log p(y|A)\n",
    "$$\n",
    "\n",
    "es la entropía del subconjunto $A$. En la expresión anterior $p(y|A)$ es la frecuencia relativa de los ejemplos de clase $y$ dentro de $A$.\n",
    "\n",
    "**La entropía mide la “pureza” del subconjunto en términos de sus clases. El subconjunto más puro es aquel donde todos los elementos son de la misma clase. El nodo más impuro es aquel en donde hay igual cantidad de elementos de cada clase (uniforme).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1790cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a7764d",
   "metadata": {},
   "source": [
    "## Algoritmo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863d92e4",
   "metadata": {},
   "source": [
    "Sea el siguiente arreglo las etiquetas de un subconjunto de 12 muestras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb4f965",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array([1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324d1f4a",
   "metadata": {},
   "source": [
    "Asumiendo que el problema sólo tiene dos clases las frecuencias relativas son"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0b1cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "4/12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ecf543",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, counts = np.unique(labels, return_counts=True)\n",
    "counts/len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db49889f",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a4df35",
   "metadata": {},
   "source": [
    "Y la entropía del conjunto sería:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e234a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(subset_labels):\n",
    "    unique, counts = np.unique(subset_labels, return_counts=True)\n",
    "    frequencies = counts/len(subset_labels)\n",
    "    return -np.sum(frequencies*np.log2(frequencies+1e-16))\n",
    "\n",
    "entropy(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54bd72f2",
   "metadata": {},
   "source": [
    "La entropía es máxima si hay igual cantidad de ejemplos de ambas clases (mínima pureza)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b36623a",
   "metadata": {},
   "outputs": [],
   "source": [
    "B = np.array([1, 1, 1, 0, 0, 0])\n",
    "entropy(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb824ea",
   "metadata": {},
   "source": [
    "y mínima si todos los ejemplos son de una clase  (máxima pureza)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf590fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "C = np.array([1, 1, 1, 1, 1, 1])\n",
    "entropy(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c83102f",
   "metadata": {},
   "source": [
    "**Extensión a más de dos clases**\n",
    "\n",
    "Si un nodo separa el conjunto en $k$ subconjuntos la regla es\n",
    "\n",
    "\n",
    "- En cada nodo se escoge el atributo que maximiza la ganancia de información.\n",
    "\n",
    "Consideremos el siguiente problema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d965f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {'tiempo': ['soleado', 'soleado', 'soleado', 'lluvioso', 'lluvioso'], \n",
    "        'humedad': ['baja', 'baja', 'alta', 'alta', 'alta'],\n",
    "        'temperatura': ['templado', 'caluroso', 'caluroso', 'templado', 'frio']}\n",
    "\n",
    "node = pd.DataFrame(data)\n",
    "node"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543d293b",
   "metadata": {},
   "source": [
    "donde queremos obtener un árbol de decisión que prediga el tiempo en función de la humedad y de la temperatura.\n",
    "\n",
    "Para decidir cual variable debe ir en el primer nodo comparamos sus ganancias de información"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bacebe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def info_gain(subset, feature):\n",
    "    subset_labels = subset[\"tiempo\"].values\n",
    "    entropy_root = entropy(subset_labels)\n",
    "    entropy_nodes = []\n",
    "    for unique_label in subset[feature].unique():\n",
    "        split = subset.loc[subset[feature] == unique_label]\n",
    "        split_labels = split[\"tiempo\"].values\n",
    "        entropy_nodes.append(entropy(split_labels)*len(split_labels)/len(subset_labels))\n",
    "    return entropy_root - sum(entropy_nodes)\n",
    "\n",
    "\n",
    "for feature in [\"humedad\", \"temperatura\"]:\n",
    "    print(f\"Ganancia de información de {feature}: {info_gain(node, feature):0.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef04efe6",
   "metadata": {},
   "source": [
    "Temperatura tiene mayor ganancia que humedad, por lo tanto el primer nodo separador utiliza temperatura.\n",
    "\n",
    "Si separamos por temperatura tenemos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3f9cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "node.loc[node[\"temperatura\"] == 'frio']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2cd392",
   "metadata": {},
   "source": [
    "En el caso `frio` se produce un nodo con un sólo ejemplo. El algoritmo no seguirá dividiendo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db026c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "node.loc[node[\"temperatura\"] == 'caluroso']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fdaadc",
   "metadata": {},
   "source": [
    "En el caso `caluroso` se produce un nodo con \"puro\". El algoritmo no seguirá dividiendo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba37311",
   "metadata": {},
   "outputs": [],
   "source": [
    "node.loc[node[\"temperatura\"] == 'templado']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c825a709",
   "metadata": {},
   "source": [
    "En el caso `templado` el nodo no es puro, debemos nuevamente escoger un atributo para separar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6509db",
   "metadata": {},
   "outputs": [],
   "source": [
    "node = node.loc[node[\"temperatura\"] == 'templado']\n",
    "for feature in [\"humedad\", \"temperatura\"]:\n",
    "    print(f\"Ganancia de información de {feature}: {info_gain(node, feature)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e07b60b",
   "metadata": {},
   "source": [
    "Por lo tanto se escoge humedad, lo cual produce dos nodos puros (con un sólo ejemplo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de63396",
   "metadata": {},
   "outputs": [],
   "source": [
    "node.loc[node[\"humedad\"] == 'baja']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe3cfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "node.loc[node[\"humedad\"] == 'alta']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85af64c",
   "metadata": {},
   "source": [
    "El algoritmo sigue separando el dataset de forma recursiva hasta que todos los nodos sean puros o hasta que se supere una profundidad máxima previamente designada.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cf4c14",
   "metadata": {},
   "source": [
    "### Train-Test-Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c6d9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(X_df, y_df, test_size):\n",
    "    if isinstance(test_size, float):\n",
    "        test_size= round(test_size*len(X_df))\n",
    "    \n",
    "    ind = X_df.index.to_list()\n",
    "    test_indices = random.sample(population=ind, k = test_size)\n",
    "    \n",
    "    X_test_df = X_df.loc[test_indices]\n",
    "    X_train_df = X_df.drop(test_indices)\n",
    "    \n",
    "    y_test_df = y_df.loc[test_indices]\n",
    "    y_train_df = y_df.drop(test_indices)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return X_train_df, X_test_df, y_train_df, y_test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547530a8",
   "metadata": {},
   "source": [
    "## Creación de la clase de Arboles de Desicion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66237cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None,*,value=None):\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.value = value\n",
    "        \n",
    "    def is_leaf_node(self):\n",
    "        return self.value is not None\n",
    "\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, min_samples_split=2, max_depth=100, n_features=None):\n",
    "        self.min_samples_split=min_samples_split\n",
    "        self.max_depth=max_depth\n",
    "        self.n_features=n_features\n",
    "        self.root=None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.n_features = X.shape[1] if not self.n_features else min(X.shape[1],self.n_features)\n",
    "        self.root = self._grow_tree(X, y)\n",
    "\n",
    "    def _grow_tree(self, X, y, depth=0):\n",
    "        n_samples, n_feats = X.shape\n",
    "        n_labels = len(np.unique(y))\n",
    "\n",
    "        # crterio de paro\n",
    "        if (depth>=self.max_depth or n_labels==1 or n_samples<self.min_samples_split):\n",
    "            leaf_value = self._most_common_label(y)\n",
    "            return Node(value=leaf_value)\n",
    "\n",
    "        feat_idxs = np.random.choice(n_feats, self.n_features, replace=False)\n",
    "\n",
    "        # el mejor corte\n",
    "        best_feature, best_thresh = self._best_split(X, y, feat_idxs)\n",
    "\n",
    "        # creando nodos hijos\n",
    "        left_idxs, right_idxs = self._split(X[:, best_feature], best_thresh)\n",
    "        left = self._grow_tree(X[left_idxs, :], y[left_idxs], depth+1)\n",
    "        right = self._grow_tree(X[right_idxs, :], y[right_idxs], depth+1)\n",
    "        return Node(best_feature, best_thresh, left, right)\n",
    "\n",
    "\n",
    "    def _best_split(self, X, y, feat_idxs):\n",
    "        best_gain = -1\n",
    "        split_idx, split_threshold = None, None\n",
    "\n",
    "        for feat_idx in feat_idxs:\n",
    "            X_column = X[:, feat_idx]\n",
    "            thresholds = np.unique(X_column)\n",
    "\n",
    "            for thr in thresholds:\n",
    "                # Calculando la ganancia de información\n",
    "                gain = self._information_gain(y, X_column, thr)\n",
    "\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    split_idx = feat_idx\n",
    "                    split_threshold = thr\n",
    "\n",
    "        return split_idx, split_threshold\n",
    "\n",
    "\n",
    "    def _information_gain(self, y, X_column, threshold):\n",
    "        # entropía del padre\n",
    "        parent_entropy = self._entropy(y)\n",
    "\n",
    "        # creando a lo hijos\n",
    "        left_idxs, right_idxs = self._split(X_column, threshold)\n",
    "\n",
    "        if len(left_idxs) == 0 or len(right_idxs) == 0:\n",
    "            return 0\n",
    "        \n",
    "        # calculando el peso promedio de entropia del hijo\n",
    "        n = len(y)\n",
    "        n_l, n_r = len(left_idxs), len(right_idxs)\n",
    "        e_l, e_r = self._entropy(y[left_idxs]), self._entropy(y[right_idxs])\n",
    "        child_entropy = (n_l/n) * e_l + (n_r/n) * e_r\n",
    "\n",
    "        # Calculando IG\n",
    "        information_gain = parent_entropy - child_entropy\n",
    "        return information_gain\n",
    "\n",
    "    def _split(self, X_column, split_thresh):\n",
    "        left_idxs = np.argwhere(X_column <= split_thresh).flatten()\n",
    "        right_idxs = np.argwhere(X_column > split_thresh).flatten()\n",
    "        return left_idxs, right_idxs\n",
    "\n",
    "    def _entropy(self, y):\n",
    "        hist = np.bincount(y)\n",
    "        ps = hist / len(y)\n",
    "        return -np.sum([p * np.log(p) for p in ps if p>0])\n",
    "\n",
    "\n",
    "    def _most_common_label(self, y):\n",
    "        counter = Counter(y)\n",
    "        value = counter.most_common(1)[0][0]\n",
    "        return value\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self._traverse_tree(x, self.root) for x in X])\n",
    "\n",
    "    def _traverse_tree(self, x, node):\n",
    "        if node.is_leaf_node():\n",
    "            return node.value\n",
    "\n",
    "        if x[node.feature] <= node.threshold:\n",
    "            return self._traverse_tree(x, node.left)\n",
    "        return self._traverse_tree(x, node.right)\n",
    "    \n",
    "    def print_tree(self, tree=None, indent=\" \"):\n",
    "        ''' Función para imprimir el arbol '''\n",
    "        \n",
    "        if not tree:\n",
    "            tree = self.root\n",
    "\n",
    "        if tree.value is not None:\n",
    "            print(tree.value)\n",
    "\n",
    "        else:\n",
    "            \n",
    "            print(\"X_\"+str(tree.feature), \"<=\", tree.threshold, \"?\")\n",
    "            print(\"%sIzquierda:\" % (indent), end=\"\")\n",
    "            self.print_tree(tree.left, indent + indent)\n",
    "            print(\"%sDerecha:\" % (indent), end=\"\")\n",
    "            self.print_tree(tree.right, indent + indent)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e9ec27",
   "metadata": {},
   "source": [
    "## Ejemplo 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1d310a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "data = datasets.load_breast_cancer()\n",
    "X, y = data.data, data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935f660a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df=pd.DataFrame(X)\n",
    "y_df=pd.DataFrame(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a64c411",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d50846",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b406b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df, X_test_df, y_train_df, y_test_df =train_test_split(X_df, y_df,0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34562ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_df.values.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9db3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTree(min_samples_split=3,max_depth=3)\n",
    "clf.fit(X_train_df.values, y_train_df.values.flatten())\n",
    "predictions = clf.predict(X_test_df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896a1ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_test, y_pred):\n",
    "    return np.sum(y_test == y_pred) / len(y_test)\n",
    "\n",
    "acc = accuracy(y_test_df.values.flatten(), predictions)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3345e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.print_tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d4aaf8",
   "metadata": {},
   "source": [
    "## Ejemplo 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a914bd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iris = pd.read_csv('Iris.csv', index_col=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b75d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8d2226",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iris['Class'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1698be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iris['Class_code']=df_iris['Class'].astype('category').cat.codes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1bde3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca63463",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df = df_iris.iloc[:,:4]\n",
    "y_df = df_iris.iloc[:,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2488408d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c52c621",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4af8767",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_df.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bd572c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df, X_test_df, y_train_df, y_test_df =train_test_split(X_df, y_df,0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b749facf",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTree(max_depth=2)\n",
    "clf.fit(X_train_df.values, y_train_df.values.flatten())\n",
    "predictions = clf.predict(X_test_df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43e5048",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4abec4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_df.values.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145502cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = accuracy(y_test_df.values.flatten(), predictions)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5e069b",
   "metadata": {},
   "source": [
    "## Usando la librería scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669a76c3",
   "metadata": {},
   "source": [
    "El módulo [`tree`](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.tree) de scikit-learn tiene implementaciones de árboles de decisión para problemas de clasificación y regresión. Nos enfocaremos en la primera.\n",
    "\n",
    "Los principales argumentos de [`DecisionTreeClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier) son:\n",
    "\n",
    "- `criterion`: El criterio que se utiliza para escoger los *splits*, las opciones son `'gini'` y `'entropy'`\n",
    "- `max_depth`: Límite para la profundidad máxima del árbol\n",
    "- `min_samples_split`: El número mínimo de ejemplos en un nodo para realizar un *split*\n",
    "- `min_samples_leaf`: El número mínimo de ejemplos que pueden estar en un nodo hoja\n",
    "- `min_impurity_decrease`: La disminución de pureza mínima en un nodo para realizar un *split*\n",
    "- `class_weight`: Permite asignar ponderación a las clases, es de utilidad si se tienen clases medianamente desbalanceadas\n",
    "- `max_features`: El número máximo de atributos a considerar en cada *split*\n",
    "\n",
    "\n",
    "Si se utilizan los argumentos (hiperparámetros) por defecto el árbol crecera hasta que sus nodos sean todos puros. Esto en general produce árboles de gran profundidad (muy capaces de sobreajustarse). \n",
    "\n",
    "Se puede limitar el tamaño de un árbol aumentando `min_samples_leaf` y/o `min_samples_split`, o disminuyendo `max_depth`.\n",
    "\n",
    "\n",
    "\n",
    "Los principales métodos son:\n",
    "\n",
    "- `predict(X)`: Retorna la clase predicha\n",
    "- `predict_proba(X)`: Retorna las probabilidades de pertenecer a cada una de las clases\n",
    "- `score(X,y)`: Retorna el *accuracy* de clasificación\n",
    "- `get_params()`: Retorna los nombres de los parámetros\n",
    "\n",
    "Además tiene algunos métodos no compartidos con otros estimadores como\n",
    "\n",
    "- `get_depth()`: Retorna la profunidad del árbol aprendido\n",
    "- `get_n_leaves()`: Retorna la cantidad de nodos hoja del árbol aprendida\n",
    "- `apply(X)`: Retorna el índice de la hoja que predice cada ejemplo\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d49f9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679eff55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "\n",
    "model = DecisionTreeClassifier(criterion='entropy')\n",
    "model.fit(X_train_df, y_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfd8898",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "help(DecisionTreeClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09127416",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_depth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620f7f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X_test_df, y_test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522effce",
   "metadata": {},
   "source": [
    "Podemos utilizar la función [`plot_tree`](https://scikit-learn.org/stable/modules/generated/sklearn.tree.plot_tree.html#sklearn.tree.plot_tree) para obtener una visualización del árbol de decisión. En cada nodo se muestra:\n",
    "\n",
    "- El atributo y umbral seleccionados.\n",
    "- El valor del criterio (índice de gini).\n",
    "- La cantidad de ejemplos que entraron al nodo.\n",
    "- La cantidad de ejemplos que entraron al nodo separados por clase (en este caso tres).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc5823a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iris.columns[:-2].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef0d20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_names =  df_iris.columns[:-2].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8279f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d70f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6), tight_layout=True)\n",
    "plot_tree(model, feature_names=X_names, ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e290bc3-cc33-415b-9a5e-a62d9df9ae22",
   "metadata": {},
   "source": [
    "### Disctrización"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba63e2d-ab66-41e6-b3a3-ecbb9525c9a9",
   "metadata": {},
   "source": [
    "La discretización por árboles de decisión consiste en usar un árbol (por ejemplo, `DecisionTreeClassifier` o `DecisionTreeRegressor`) para encontrar automáticamente los puntos de corte óptimos de una variable continua, de modo que los intervalos resultantes sean informativos respecto a una variable objetivo (clasificación o regresión).\n",
    "\n",
    "Esto es útil para:\n",
    "- Transformar variables continuas en categorías basadas en su relación con la variable objetivo.\n",
    "- Capturar relaciones no lineales y umbrales relevantes para el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5630c90a-cb1d-4d13-9528-ae5ac42711cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "df=pd.read_csv('dataKmeans.csv')\n",
    "plt.scatter(df.x,df.y,s=5)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7337154-0919-47d4-aaed-3b5400f63037",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor,plot_tree  # Discretización por árboles de decisión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8732e3d9-a73e-4f2d-8618-b15c902fbb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo con datos sin discretizar\n",
    "lin_SD=LinearRegression()\n",
    "lin_SD.fit(df[['x']],df['y'])\n",
    "predict_SD=lin_SD.predict(df[['x']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599dee8b-27b5-43ed-8061-6d7b41014d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Discretización por KMeans\n",
    "# Centroides\n",
    "k=3\n",
    "ctr=np.random.uniform(df.x.min(),df.x.max(),k)\n",
    "ctr_anterior=np.ones(k)*np.inf\n",
    "eps=1e-6\n",
    "while(np.abs(ctr-ctr_anterior).sum()>eps): # Minkowski con p=1\n",
    "    dif=[]\n",
    "    for c_i in ctr:\n",
    "        dif.append(np.abs(c_i-df[['x']].values))\n",
    "    distancias=np.concatenate(dif,axis=1)\n",
    "    grupos=np.argmin(distancias,axis=1)\n",
    "    df_copia=df.copy()\n",
    "    df_copia['kmeans']=grupos\n",
    "    ctr_anterior=ctr.copy()\n",
    "    ctr=df_copia.groupby('kmeans')['x'].mean().values\n",
    "df['kmeans']=grupos\n",
    "df.groupby('kmeans')['x'].hist(bins='auto')\n",
    "ctr\n",
    "\n",
    "# Modelo con datos discretizados con K-means\n",
    "lin_kmeans=LinearRegression()\n",
    "x=ctr.reshape(-1,1)\n",
    "y=df.groupby('kmeans')['y'].mean()\n",
    "lin_kmeans.fit(x,y)\n",
    "predict_kmeans=lin_kmeans.predict(df[['x']].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2df6af-5651-459e-ba63-73faf89325c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Discretización por Arboles de decisión\n",
    "k=3\n",
    "disc_tree=DecisionTreeRegressor(max_leaf_nodes=k)\n",
    "disc_tree.fit(df[['x']],df['y'])\n",
    "df['tree']=disc_tree.predict(df[['x']])\n",
    "df['tree'].unique(),df.groupby('tree')['y'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c700d2-af87-4539-a81c-f2f4949875d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_tree=LinearRegression()\n",
    "x=df.groupby('tree')['x'].mean().values.reshape(-1,1)\n",
    "lin_tree.fit(x,df['tree'].unique())\n",
    "predict_tree=lin_tree.predict(df[['x']].values)\n",
    "\n",
    "plt.scatter(df.x,df.y,s=5,c='k')\n",
    "plt.plot(df.x,predict_SD,'g',lw=5,label='Regresión sin discretizar')\n",
    "plt.plot(df.x,predict_kmeans,'--r',label='Regresión con kmeans',lw=3)\n",
    "plt.plot(df.x,predict_tree,':m',label='Regresión con árboles',lw=3)\n",
    "plt.legend()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e1d5d5-fe46-4b4e-a565-d3ef3cccd323",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_SD.coef_,lin_SD.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963c88c5-b6f8-4755-a085-b1dcd86e4279",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_tree.coef_,lin_tree.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88b7609-8216-4496-8c4b-7d171742fecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df.x,df.y,s=5)\n",
    "plt.scatter(df.x,df.tree,s=5)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2fe578-4078-4f89-97ed-77a20f3ac4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tree(disc_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d416660-ba79-44fd-b6e0-95267c062b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df.x,df.y,s=5,c='k')\n",
    "for h_i in df['tree'].unique():\n",
    "    x=df[df['tree']==h_i][['x']]\n",
    "    y=df[df['tree']==h_i]['y']\n",
    "    lin_model = LinearRegression()\n",
    "    lin_model.fit(x.values, y.values)\n",
    "    predict_lin = lin_model.predict(x)\n",
    "    #plt.plot(x,lin_grupos[-1].predict(x),label='Grupo '+str(round(h_i,2)))\n",
    "    plt.plot(x.values, predict_lin,label='Grupo '+str(round(h_i,2)))\n",
    "plt.legend()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfb1d91-dc4a-4f9b-97bb-be9ee3c59899",
   "metadata": {},
   "source": [
    "La discretización de datos se utiliza para acelerar el tiempo de entrenamiento de los modelos basados ​​en árboles de decisiones, reducir el impacto de valores atípicos en los modelos lineales y hacer que los datos sean más comprensibles para las personas.\n",
    "\n",
    "El desafío en la discretización es encontrar los límites de intervalo que maximizan el rendimiento del modelo y minimizan los tiempos de entrenamiento y la pérdida de información."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26d0ec4-68a3-4a18-8a4f-2ffa9a1c83c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
